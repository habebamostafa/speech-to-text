# -*- coding: utf-8 -*-
"""Speech-to-text.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XLt5O5w-9yKuKT17OVwzSQqFN0CsI5f9

<a href="https://colab.research.google.com/github/Sukalp-Jhingran/Speech-to-text-Convertor/blob/main/Untitled9.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>
"""

pip install jiwer ft

from jiwer import wer

ref = "hello world"
hyp = "hello duck"
print("WER:", wer(ref, hyp))

!pip install tensorflow

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import matplotlib.pyplot as plt
from IPython import display
from jiwer import wer

dataset_url = "https://keithito.com/LJ-Speech-Dataset/"

from tensorflow import keras

data_url = "https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2"
data_path = keras.utils.get_file(
    fname="LJSpeech-1.1",
    origin=data_url,
    extract=True,
    archive_format='tar',
    cache_dir='.',
)

import tarfile
import os
import urllib.request

url = "https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2"
download_path = "LJSpeech-1.1.tar.bz2"
if not os.path.exists(download_path):
    urllib.request.urlretrieve(url, download_path)

extracted_dir = "LJSpeech-1.1"
if not os.path.exists(extracted_dir):
    with tarfile.open(download_path, "r:bz2") as tar:
        tar.extractall()

import pandas as pd
metadata_path = os.path.join(extracted_dir, "metadata.csv")
metadata_df = pd.read_csv(metadata_path, sep="|", header=None, quoting=3)

wavs_path = extracted_dir + "/wavs/"
metadata_path = data_path + "/metadata.csv"

import csv

metadata_path = "datasets/LJSpeech-1.1/metadata.csv"

print(metadata_df.head(10))

metadata_df.tail()

metadata_df.columns = ["file_name", "transcription", "normalized_transcription"]
metadata_df = metadata_df[["file_name","normalized_transcription"]]
metadata_df = metadata_df.sample(frac=1).reset_index(drop=True)
metadata_df.head(3)

split = int(len(metadata_df) * 0.90)
df_train = metadata_df[:split]
df_val = metadata_df[split: ]
print(f"Size of the training set: {len(df_train)}")
print(f"Size of the training set: {len(df_val)}")

"""**Preprocessing**


---

We first prepare the vocabulary to be used.
"""

# The set of characters accepted in the transcription
characters = [x for x in "abcdefghijklmnopqrstuvwxyz'?! "]

# Mapping characters to integers
char_to_num = keras.layers.StringLookup(vocabulary=characters, oov_token="")

# Mapping integers back to original characters
num_to_char = keras.layers.StringLookup(
    vocabulary=char_to_num.get_vocabulary(), oov_token="", invert=True
)

# Print vocabulary and size
print(
    f"The vocabulary is: {char_to_num.get_vocabulary()} "
    f"(size = {char_to_num.vocabulary_size()})"
)

char_to_num

# Constants
frame_length = 256
frame_step = 160
fft_length = 384

def encode_single_sample(wav_file, label):


    # 1. Read wav file
    file = tf.io.read_file(wavs_path + wav_file + ".wav")

    # 2. Decode the wav file
    audio, _ = tf.audio.decode_wav(file)
    audio = tf.squeeze(audio, axis=-1)

    # 3. Change type to float
    audio = tf.cast(audio, tf.float32)

    # 4. Get the spectrogram
    spectrogram = tf.signal.stft(
        audio,
        frame_length=frame_length,
        frame_step=frame_step,
        fft_length=fft_length
    )

    # 5. Get magnitude and apply root compression
    spectrogram = tf.abs(spectrogram)
    spectrogram = tf.math.pow(spectrogram, 0.5)

    # 6. Normalize
    means = tf.math.reduce_mean(spectrogram, 1, keepdims=True)
    stddevs = tf.math.reduce_std(spectrogram, 1, keepdims=True)
    spectrogram = (spectrogram - means) / (stddevs + 1e-10)

    # 7. Process the label
    label = tf.strings.lower(label)
    label = tf.strings.unicode_split(label, input_encoding="UTF-8")
    label = char_to_num(label)

    # 8. Return spectrogram and encoded label
    return spectrogram, label

batch_size=32
# define the training dataset
train_dataset=tf.data.Dataset.from_tensor_slices(
    (list(df_train["file_name"]),list(df_train["normalized_transcription"]))
)
train_dataset=(
    train_dataset.map(encode_single_sample,num_parallel_calls=tf.data.AUTOTUNE)
    .padded_batch(batch_size)
    .prefetch(buffer_size=tf.data.AUTOTUNE)
)
# define the validation dataset
validation_dataset=tf.data.Dataset.from_tensor_slices(
    (list(df_val["file_name"]),list(df_val["normalized_transcription"]))
)
validation_dataset=(
    validation_dataset.map(encode_single_sample,num_parallel_calls=tf.data.AUTOTUNE)
    .padded_batch(batch_size)
    .prefetch(buffer_size=tf.data.AUTOTUNE)
)

fig = plt.figure(figsize=(8, 5))
for batch in train_dataset.take(1):
    spectogram = batch[0][0].numpy().T  # transpose only
    label = batch[1][0]
    label = tf.strings.reduce_join(num_to_char(label)).numpy().decode("utf-8")

    # Plot spectrogram
    ax = plt.subplot(2, 1, 1)
    ax.imshow(spectogram, vmax=1, aspect='auto', origin='lower')
    ax.set_title(label)
    ax.axis("off")

    # Plot waveform
file_name = list(df_train["file_name"])[0]
file = tf.io.read_file(wavs_path + file_name + ".wav")
audio, _ = tf.audio.decode_wav(file)
audio = audio.numpy()
audio = np.squeeze(audio)
audio = np.clip(audio, -1.0, 1.0)  #  Fix for display.Audio

ax = plt.subplot(2, 1, 2)
plt.plot(audio)
ax.set_title("Signal Wave")
ax.set_xlim(0, len(audio))

display.display(display.Audio(audio, rate=16000))

def CTCLoss(y_true,y_pred):
  # compute the training-time loss value
  batch_len=tf.cast(tf.shape(y_true)[0],dtype="int64")
  input_length=tf.cast(tf.shape(y_true)[0],dtype="int64")
  label_length=tf.cast(tf.shape(y_true)[0],dtype="int64")

  input_length=input_length* tf.ones(shape=(Batch_len,1), dtype="int64")
  label_length=label_length* tf.ones(shape=(Batch_len,1), dtype="int64")

  loss=keras.backend.ctc_batch_cost(y_true,y_pred,input_length,label_length)
  return loss

def build_model(input_dim,output_dim,rnn_layers=5,rnn_units=128):
  # model similar to deepspeech2
  input_spectrogram=layers.Input((None,input_dim),name="input")
  x=layers.Reshape((-1,input_dim,1),name="expand_dim")(input_spectrogram)
  x=layers.Conv2D(
      filters=32,
      kernel_size=[11,41],
      strides=[2,2],
      padding="same",
      use_bias=False,
      name="conv_1",
  )(x)
  x=layers.BatchNormalization(name="conv_1_bn")(x)
  x=layers.ReLU(name="conv_1_relu")(x)
  # convulation layer 2
  x=layers.Conv2D(
      filters=32,
      kernel_size=[11,21],
      strides=[1,2],
      padding="same",
      use_bias=False,
      name="conv_2",
  )(x)
  x=layers.BatchNormalization(name="conv_2_bn")(x)
  x=layers.ReLU(name="conv_2_relu")(x)
  # reshape the resulted volume to feed the rnns layers
  x=layers.Reshape((-1,x.shape[-2]*x.shape[-1]))(x)
  # RNN layers
  for i in range(1,rnn_layers+1):
    recurrent=layers.GRU(
        units=rnn_units,
        activation="tanh",
        recurrent_activation="sigmoid",
        use_bias=True,
        return_sequences=True,
        reset_after=True,
        name=f"gru_{i}",
    )
    x=layers.Bidirectional(
        recurrent,name=f"bidirectional_{i}",merge_mode="concat"
    )(x)
    if i<rnn_layers:
      x=layers.Dropout(rate=0.5)(x)
    # dense layer
    x=layers.Dense(units=rnn_units*2,name=f"dense_1",)(x)
    x=layers.ReLU(name=f"dense_1_relu")(x)
    x=layers.Dropout(rate=0.5)(x)
    # classification layer
    output=layers.Dense(units=output_dim+1,activation="softmax")(x)
    # model
    model=keras.Model(input_spectrogram,output,name="DeepSpeech_2")
    # Optimizer
    opt=keras.optimizers.Adam(learning_rate=1e-4)
    # Compile the model and return
    model.compile(optimizer=opt,loss=CTCLoss)
    return model

# get the model
model=build_model(
    input_dim=fft_length//2+1,
    output_dim=char_to_num.vocabulary_size(),
    rnn_units=512,
)
model.summary(line_length=110)

# A utility function to decode the output of the network
def decode_batch_predictions(pred):
    input_len = np.ones(pred.shape[0]) * pred.shape[1]
    # Use greedy search
    results = keras.backend.ctc_decode(pred, input_length=input_len, greedy=True)[0][0]

    # Iterate over the results and get back the text
    output_text = []
    for result in results:
        result = tf.strings.reduce_join(num_to_char(result)).numpy().decode("utf-8")
        output_text.append(result)
    return output_text

# A callback class to output a few transcriptions during training
class CallbackEval(keras.callbacks.Callback):
    def __init__(self, dataset):
        super().__init__()
        self.dataset = dataset

    def on_epoch_end(self, epoch, logs=None):
        predictions = []
        targets = []
        for batch in self.dataset:
            X, y = batch
            batch_predictions = model.predict(X)
            batch_predictions = decode_batch_predictions(batch_predictions)
            predictions.extend(batch_predictions)
            for label in y:
                label = tf.strings.reduce_join(num_to_char(label)).numpy().decode("utf-8")
                targets.append(label)

        wer_score = wer(targets, predictions)
        print("-" * 100)
        print(f"Word Error Rate: {wer_score:.4f}")
        print("-" * 100)

        for i in np.random.randint(0, len(predictions), 2):
            print(f"Target:     {targets[i]}")
            print(f"Prediction: {predictions[i]}")
            print("-" * 100)

def CTCLoss(y_true, y_pred):
    # y_pred: (batch_size, time_steps, vocab_size)
    batch_len = tf.cast(tf.shape(y_true)[0], dtype="int64")
    input_length = tf.cast(tf.shape(y_pred)[1], dtype="int64")
    label_length = tf.cast(tf.shape(y_true)[1], dtype="int64")

    input_length = input_length * tf.ones(shape=(batch_len, 1), dtype="int64")
    label_length = label_length * tf.ones(shape=(batch_len, 1), dtype="int64")

    loss = keras.backend.ctc_batch_cost(y_true, y_pred, input_length, label_length)
    return loss

model.compile(optimizer=keras.optimizers.Adam(), loss=CTCLoss)

# تفعيل GPU وتحسين الإعدادات
import tensorflow as tf
from tensorflow import keras
import os

# تفعيل GPU
print("TensorFlow version:", tf.__version__)
print("GPU Available: ", tf.config.list_physical_devices('GPU'))

# إعدادات الذاكرة للنموذج
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        # السماح بالنمو التدريجي للذاكرة
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
        logical_gpus = tf.config.experimental.list_logical_devices('GPU')
        print(f"{len(gpus)} Physical GPUs, {len(logical_gpus)} Logical GPUs")
    except RuntimeError as e:
        print(e)

# تحسين أداء البيانات
AUTOTUNE = tf.data.AUTOTUNE

# تعديل دالة encode_single_sample لتعمل بشكل أفضل مع GPU
@tf.function
def encode_single_sample(wav_file, label):
    # الكود الأصلي للدالة هنا
    file = tf.io.read_file(wavs_path + wav_file + ".wav")
    audio, _ = tf.audio.decode_wav(file)
    audio = tf.squeeze(audio, axis=-1)
    audio = tf.cast(audio, tf.float32)

    spectrogram = tf.signal.stft(
        audio,
        frame_length=frame_length,
        frame_step=frame_step,
        fft_length=fft_length
    )

    spectrogram = tf.abs(spectrogram)
    spectrogram = tf.math.pow(spectrogram, 0.5)

    means = tf.math.reduce_mean(spectrogram, 1, keepdims=True)
    stddevs = tf.math.reduce_std(spectrogram, 1, keepdims=True)
    spectrogram = (spectrogram - means) / (stddevs + 1e-10)

    label = tf.strings.lower(label)
    label = tf.strings.unicode_split(label, input_encoding="UTF-8")
    label = char_to_num(label)

    return spectrogram, label

# تحسين datasets
train_dataset = tf.data.Dataset.from_tensor_slices(
    (list(df_train["file_name"]), list(df_train["normalized_transcription"]))
)
train_dataset = (
    train_dataset.map(encode_single_sample, num_parallel_calls=AUTOTUNE)
    .padded_batch(batch_size)
    .prefetch(buffer_size=AUTOTUNE)
)

validation_dataset = tf.data.Dataset.from_tensor_slices(
    (list(df_val["file_name"]), list(df_val["normalized_transcription"]))
)
validation_dataset = (
    validation_dataset.map(encode_single_sample, num_parallel_calls=AUTOTUNE)
    .padded_batch(batch_size)
    .prefetch(buffer_size=AUTOTUNE)
)

print("Datasets optimized for GPU")

# التحقق من استخدام GPU
import tensorflow as tf
print("Devices available:")
for device in tf.config.list_physical_devices():
    print(f" - {device}")

# اختبار بسيط على GPU
with tf.device('/GPU:0'):
    a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])
    b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])
    c = tf.matmul(a, b)
    print("Matrix multiplication result:")
    print(c)
    print("Operation executed on:", c.device)

import tensorflow as tf
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau

# إعدادات التدريب المحسنة لـ GPU
epochs = 5

# callbacks إضافية لتحسين التدريب
callbacks = [
    validation_callback,
    ModelCheckpoint(
        'best_model.h5',
        monitor='val_loss',
        save_best_only=True,
        save_weights_only=False,
        mode='min',
        verbose=1
    ),
    EarlyStopping(
        monitor='val_loss',
        patience=3,
        restore_best_weights=True,
        verbose=1
    ),
    ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,
        patience=2,
        min_lr=1e-7,
        verbose=1
    ),
    tf.keras.callbacks.TensorBoard(
        log_dir='./logs',
        histogram_freq=1
    )
]

# التدريب مع إعدادات محسنة لـ GPU
history = model.fit(
    train_dataset,
    epochs=epochs,
    validation_data=validation_dataset,
    callbacks=callbacks,
    verbose=1,
    # إعدادات إضافية لتحسين الأداء
)

# epochs = 5
# validation_callback = CallbackEval(validation_dataset)

# history = model.fit(
#     train_dataset,
#     validation_data=validation_dataset,
#     epochs=epochs,
#     callbacks=[validation_callback],
# )



predictions=[]
targets=[]
for batch in validation_dataset:
  X,y=batch
  batch_predictions=model.predict(X)
  batch_predictions=decode_batch_predictions(batch_predictions)
  predictions.extend(batch_predictions)
  for label in y:
    label=tf.strings.reduce_join(num_to_char(label)).numpy().decode("utf-8")
    targets.append(label)
wer_score=wer(targets,predictions)
print("-"*100)
print(f"Word Error Rate: {wer_score:.4f}")
print("-"*100)
for i in np.random.randint(0,len(predictions),5):
  print(f"Target: {targets[i]}")
  print(f"Prediction: {predictions[i]}")
  print("-"*100)

!pip install rouge-score
import numpy as np
import tensorflow as tf
from jiwer import wer, cer
from nltk.translate.bleu_score import corpus_bleu
from rouge_score import rouge_scorer

predictions = []
targets = []

# Collect predictions and ground-truth labels
for batch in validation_dataset:
    X, y = batch
    batch_predictions = model.predict(X)
    batch_predictions = decode_batch_predictions(batch_predictions)
    predictions.extend(batch_predictions)

    for label in y:
        label = tf.strings.reduce_join(num_to_char(label)).numpy().decode("utf-8")
        targets.append(label)

# -------------------------
# Evaluation Metrics
# -------------------------

# 1. Word Error Rate (WER)
wer_score = wer(targets, predictions)

# 2. Character Error Rate (CER)
cer_score = cer(targets, predictions)

# 3. BLEU Score (corpus-level, n-gram overlap)
# BLEU expects tokenized references and candidates
references = [[t.split()] for t in targets]   # list of lists of reference tokens
candidates = [p.split() for p in predictions] # list of hypothesis tokens
bleu_score = corpus_bleu(references, candidates)

# 4. ROUGE-L (longest common subsequence)
scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)
rouge_scores = [scorer.score(t, p)['rougeL'].fmeasure for t, p in zip(targets, predictions)]
avg_rougeL = np.mean(rouge_scores)

# -------------------------
# Print Results
# -------------------------
print("="*100)
print(f"Word Error Rate (WER): {wer_score:.4f}")
print(f"Character Error Rate (CER): {cer_score:.4f}")
print(f"BLEU Score: {bleu_score:.4f}")
print(f"Average ROUGE-L F1: {avg_rougeL:.4f}")
print("="*100)

# Show random examples
for i in np.random.randint(0, len(predictions), 5):
    print(f"Target: {targets[i]}")
    print(f"Prediction: {predictions[i]}")
    print("-"*100)





# Save full model
model.save("my_model.h5")   # or model.save("my_model.keras")



model.save("my_model.h5")
print("Saved model to disk")

from google.colab import drive
drive.mount('/content/drive')

# Save to Drive
model.save('/content/drive/MyDrive/my_model.h5')  # TensorFlow

